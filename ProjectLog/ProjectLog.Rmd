---
title: "Predicting ten year risk of coronary heart disease - Project Log"
author: "Dennis Wiersma"
date: "`r Sys.Date()`"
header-includes:
  - \usepackage{multirow}
  - \usepackage{nameref}
  - \usepackage{float}
  - \floatplacement{figure}{H}
output: 
  pdf_document: 
    number_sections: yes
    highlight: kate
urlcolor: blue
---

```{r, include=FALSE}
# Copyright (c) 2022 Dennis Wiersma.
# Licensed under GPLv3. See LICENSE file.
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache = TRUE)
```

```{r libraries, include=FALSE}
# Loading required libraries
library("ggplot2")
library("ggpubr")
library("ggbiplot")
library("corrplot")
```

```{=tex}
\newpage
\tableofcontents
\newpage
\listoffigures
\listoftables
\newpage
```
# Exploratory Data Analysis

Cardiovascular disease gets more prominent by the day. As one of the leading causes of death in the developed world it is an important issue to examine. Getting to know more about this matter and what factors impact risk of complications can therefore be rather interesting. This chapter describes the Exploratory Data Analysis which functions as a kick off point for this project.

## Dataset

[The dataset used in this research project](https://www.kaggle.com/datasets/dileep070/heart-disease-prediction-using-logistic-regression) contains a subset of data gathered from an ongoing cardiovascular study on residents of the town of Framingham, Massachusetts, United States of America. This study is better known as the [Framingham Heart Study](https://www.framinghamheartstudy.org/) (FHS).

### Loading the data

The subset of the large collection of data used for this research contains over 4000 records and 15 different variables. In this section all necessary files will be loaded for analysis.

#### Loading the codebook

The codebook is a document describing variables and listing additional information like units and datatypes.\
The codebook will be loaded as follows:

```{r loadingCodebook}
# Load codebook
codeBook <- read.table(file = "../CodeBook.txt", header = TRUE, sep = "|")
# Create table w/ codebook contents
knitr::kable(codeBook, "simple", caption = "Codebook with variable descriptions.")
```

#### Loading the dataset

To analyse our dataset we'll of course need access to it. It will be loaded with the code below:

```{r loadingDataset}
# Load dataset
dataset <- read.table(file = "../data/framingham.csv", header = TRUE, sep = ",")
# Get dimensions of dataset
dataset.dim <- dim(dataset)
# Print dimensions of dataset
cat("Instances: ", dataset.dim[1], "\nvariables: ", dataset.dim[2])
```

#### Dataset inconsistencies

The dataset mentioned above was found on the [Kaggle](https://www.kaggle.com/datasets) website. There are a few problems with this data. First off there is the "education" variable. This variable consists of values 1 through 4, presumably to indicate education levels. However, what these numbers represent is not explained anywhere in the dataset's description. Thus far it is still unclear what exactly these values specify.\
An example of the education variable:

```{r educationExample}
# Get head of education column and print table
knitr::kable(head(dataset["education"]), "simple", caption = "First six values in the education column of the dataset.")
```

Also included in this dataset are some numeric variables, described on the dataset's page as "continuous". These values all lack descriptions of their units. Where a variable's unit can sometimes easily be inferred, like with heart rate, other times it means we end up with no unit whatsoever.\
The following variables have missing units:

```{r missingUnitList}
# Print codebook entries where unit is NA
knitr::kable(codeBook[which(is.na(codeBook["unit"])), c("name", "unit")], caption = "List of variables that lack units.")
```

To resolve these missing units one may look for the source of the data. The source mentioned on the Kaggle page links to another page on Kaggle. The problem is that this page no longer exists. We do however know that this data originates from the [Framingham Heart Study](https://www.framinghamheartstudy.org/) (FHS). Searching through this website for our missing units led to [this page](https://www.framinghamheartstudy.org/fhs-for-researchers/data-available-overview/) about available FHS data. From there we find a page on [FHS Dataset Inventory for Public Viewing](https://wwwapp.bumc.bu.edu/FHSresapp/Public/DataSetList) which lists all datasets which can be requested from the FHS team. This page includes documentation on variables. This sounds like progress but there are fifty pages of datasets to choose from. Finding out which of these datasets was used to source the data used in this project would be too time intensive. Therefore, there is no other choice but to work with what we have and use the data without it's corresponding units.

### Datatype correction

```{r structure}
str(dataset)
```

When looking at the structure of our dataset above one might notice the fact that some of the data types aren't quite right. For example, a lot of our nominal values are encoded as integers. We shall fix this now:

```{r fixDatatypes}
# Change male column from int to female/male factor
dataset$male <- factor(dataset$male, labels = c("female", "male"))
# Change currentSmoker column from int to no/yes factor
dataset$currentSmoker <- factor(dataset$currentSmoker, labels = c("no", "yes"))
# Change BPMeds column from int to no/yes factor
dataset$BPMeds <- factor(dataset$BPMeds, labels = c("no", "yes"))
# Change prevalentStroke column from int to no/yes factor
dataset$prevalentStroke <- factor(dataset$prevalentStroke, labels = c("no", "yes"))
# Change prevalentHyp column from int to no/yes factor
dataset$prevalentHyp <- factor(dataset$prevalentHyp, labels = c("no", "yes"))
# Change diabetes column from int to no/yes factor
dataset$diabetes <- factor(dataset$diabetes, labels = c("no", "yes"))
# Change TenYearCHD column from int to no/yes factor
dataset$TenYearCHD <- factor(dataset$TenYearCHD, labels = c("no", "yes"))
```

If we look at the data's structure again now, we'll see it looks a lot better.

```{r structure2}
str(dataset)
```

### Missing values

Before we can continue to analysing the dataset with fancy distributions and the likes, we have to decide how to deal with missing values in our dataset. Depending on the amount and type of the missing data there are several methods of approach. Let's take a look at how many missing values each of our variables contains:

```{r NAvalues}
# Get the 7th row of the dataset's summary (the NA's row)
# Then print the rows where number of NA's > 0
knitr::kable(summary(dataset)[7, which(!is.na(summary(dataset)[7,]))], "simple", caption = "Number of missing values for each variable that contains missing values.")
```

#### Education

First up is education. We still don't know what the education variable really even means, but one could reasonably assume a lower number to correspond to a lower level of education. However, this approach would be unreliable at best and actively harmful to our conclusions at worst. Therefore it would be best to deal with these missing values by simply getting rid of the entire variable, which we shall do now:

```{r dropEducation}
# Remove the education column from the dataset
dataset <- subset(dataset, select = -c(education))
```

#### cigsPerDay - Cigarettes smoked per day

This variable only has a few dozen missing values, so it wouldn't be the end of the world to simply get rid of them. There is a better way however. Let's first take a look at these missing values and check whether all of the missing cigsPerDay values are actually from smokers:

```{r cigsPerDayNATable}
knitr::kable(dataset[which(is.na(dataset["cigsPerDay"])), c("currentSmoker", "cigsPerDay")], caption = "All instances of cigsPerDay that are missing values.")
```

As can be seen in the table above, all of the present NA values are indeed from current smokers. An easy fix for these missing values would be to assign them the median amount of cigarettes smoked per day *by current smokers*, so that is what we'll do next. We'll also check the number of NA values before and after dealing with them as a check.

```{r cigsPerDayNA}
# Check number of NAs
cat("Number of NAs before: ", sum(is.na(dataset["cigsPerDay"])))
# Calculate median cigarettes per day for smokers
medianCigsPerDay <- median(na.omit(dataset["cigsPerDay"][dataset["currentSmoker"] == "yes"]))
# Change number of cigarettes per day to median
dataset[which(is.na(dataset["cigsPerDay"])), "cigsPerDay"] <- medianCigsPerDay
# Check number of NAs
cat("Number of NAs after: ", sum(is.na(dataset["cigsPerDay"])))
```

#### BPMeds - Blood pressure medication

To decide what to do with the missing blood pressure medication values, we shall first look at how balanced this variable is. One might reasonably assume that subjects that are on blood pressure medication would be in the minority, but it's still worth it to check.

```{r BPMedsNAPlot, warning=FALSE}
ggplot(dataset, aes(x = BPMeds, fill = BPMeds)) + 
  geom_bar() +
  ggtitle("Subjects on blood pressure medication")
```

From this plot we may conclude that a large majority of subjects are in fact not on blood pressure medication. We can therefore assume that subjects with missing values for this variable aren't on blood pressure medication either. So let's fix the values:

```{r BPMedsNA}
# Check number of NAs
cat("Number of NAs before: ", sum(is.na(dataset["BPMeds"])))
# Change missing BPMeds values to "no"
dataset[which(is.na(dataset["BPMeds"])), "BPMeds"] <- "no"
# Check number of NAs
cat("Number of NAs after: ", sum(is.na(dataset["BPMeds"])))
```

#### totChol - Total cholestorol level

Since total cholestorol level is a simple integer variable instead of a nominal variable we can just assign the missing values the median of all the other values.

```{r totCholNA}
# Check number of NAs
cat("Number of NAs before: ", sum(is.na(dataset["totChol"])))
# Calculate median totChol
medianTotChol <- median(na.omit(dataset$totChol))
# Change missing totChol values to median
dataset[which(is.na(dataset["totChol"])), "totChol"] <- medianTotChol
# Check number of NAs
cat("Number of NAs after: ", sum(is.na(dataset["totChol"])))
```

#### BMI - Body Mass Index

Since BMI is a simple integer variable instead of a nominal variable we can just assign the missing values the median of all the other values.

```{r BMINA}
# Check number of NAs
cat("Number of NAs before: ", sum(is.na(dataset["BMI"])))
# Calculate median BMI
medianBMI <- median(na.omit(dataset$BMI))
# Change missing BMI values to median
dataset[which(is.na(dataset["BMI"])), "BMI"] <- medianBMI
# Check number of NAs
cat("Number of NAs after: ", sum(is.na(dataset["BMI"])))
```

#### heartRate - Heart rate

Since heart rate level is a simple integer variable instead of a nominal variable we can just assign the missing values the median of all the other values.

```{r totHeartRateNA}
# Check number of NAs
cat("Number of NAs before: ", sum(is.na(dataset["heartRate"])))
# Calculate median heartRate
medianHeartRate<- median(na.omit(dataset$heartRate))
# Change missing heartRate values to median
dataset[which(is.na(dataset["heartRate"])), "heartRate"] <- medianHeartRate
# Check number of NAs
cat("Number of NAs after: ", sum(is.na(dataset["heartRate"])))
```

#### glucose - Glucose levels

Since glucose levels is a simple integer variable instead of a nominal variable we can just assign the missing values the median of all the other values.

```{r glucoseNA}
# Check number of NAs
cat("Number of NAs before: ", sum(is.na(dataset["glucose"])))
# Calculate median glucose
medianGlucose <- median(na.omit(dataset$glucose))
# Change missing glucose values to median
dataset[which(is.na(dataset["glucose"])), "glucose"] <- medianGlucose
# Check number of NAs
cat("Number of NAs after: ", sum(is.na(dataset["glucose"])))
```

## Visualisations

Now that we have a complete and workable dataset we can start exploring what our data actually looks like. There's a collection of different plots and figures we'll use to explore our data. However, we are limited in which variables we can use with each analysis. The main differentiating factor here would be whether a variable is numeric or nominal. Certain visualisations cannot be done on nominal data. For example it isn't very useful to compare two factors in a boxplot. We will therefore, on occasion, explore these different types of variables in their own distinct ways.

### Boxplots split on class variable

Let us start by looking at the variations in our data. To do this we'll be creating a boxplot for every one of our numeric variables. This will let us examine the variance, as well as some key metrics like the mean, in an easy to grasp visual manner. We split the boxplots up into two groups per variable based on the `TenYearCHD` variable. This let's us get a sense of whether our class variable has an impact on the numeric variable at hand.

```{r boxplots, fig.cap="Collection of boxplots for each numeric variable, split on class variable."}
# function that creates a boxplot on dataset given a column name
boxplots <- function(colName) {
  ggplot(dataset, aes(x = TenYearCHD, y = !!sym(colName), fill = TenYearCHD)) +
  geom_boxplot(show.legend = FALSE) +
  theme(text=element_text(size=8))
}

# vector with all numeric columns
numeric_cols <- c("age", "cigsPerDay", "totChol", "sysBP", "BMI", "heartRate", "glucose")
# execute boxplots function on every numeric column
plot_list <- lapply(numeric_cols, boxplots)
# arrange these boxplots into one figure
ggarrange(plotlist = plot_list, ncol = 4, nrow = 2)
```

As you may have noticed when exploring these plots, we are presented with a lot of outliers as well as some skewed data. The `glucose` variable would be a great example of the former, while the `cigsPerDay` variable is a wonderful illustration of the latter. We'll try to remedy this in our next step.

### Boxplots split on class variable - Log transformation

To try and reduce both the outliers in our data as well as the skewness, we will perform a log transformation which we shall then plot in the same way as in the last step. This should, theoretically, help to reduce both of these problems.\
When performing this operation we do not just use the `log()` function, but we add one to all of the values first. To explain why we will once again look at the `cigsPerDay` variable. As you might imagine there are a lot of people that simply don't smoke and will therefore have reported zero cigarettes smoked. If we were to perform a log transformation now, R would return infinity for all the zero values. These infinity values would then automatically be filtered out by R when trying to create a plot, therefore simply removing these data points from the visualisation. If we add one to all of our values the distribution stays the same, but it does eliminate this problem of infinite values.

```{r logTransformedBoxplots,fig.height=4 , fig.cap="Collection of boxplots for each log transformed numeric variable, split on class variable."}
# function that creates a boxplot on log of dataset given a column name
boxplotsLogTransform <- function(colName) {
  ggplot(dataset, aes(x = TenYearCHD, y = log(!!sym(colName) + 1), fill = TenYearCHD)) +
  geom_boxplot(show.legend = FALSE) +
  theme(text=element_text(size=8))
}

# execute boxplots function on every numeric column
plot_list <- lapply(numeric_cols, boxplotsLogTransform)
# arrange these boxplots into one figure
ggarrange(plotlist = plot_list, ncol = 4, nrow = 2)
```

Let's take a look at the same two variables as in our last step, `glucose` and `cigsPerDay`. Examining `glucose`'s boxplot we learn that the boxplot itself has gotten a little larger, indicating fewer outliers. The difference isn't all that noticeable however, and we are still presented with a lot of outliers. Evaluating the `cigsPerDay` variable presents us with a disparity between the two `TenYearCHD` categories. While the `yes` boxplot clearly shows an improvement in its skewness, the `no` boxplot does not.\
Overall this transformation doesn't seem to have had too much of an impact. We therefore won't be using it any further, unless it turns out to be preferable when selecting an algorithm later on in this project.

### Class distributions
\label{1.2.3}

Now that we know about the variations and outliers in our data, it is time to look at it's distribution. At this step we can look at both our numeric as well as our categorical data since were looking at a collection of bar plots with frequencies.\
For every one of our variables we'll make one of these bar plots. We can use these to look for two features in particular. First we can quickly glance at the bar plots for our numeric data and speculate whether they seem to be normally distributed. Secondly we may examine all of the bar plots a little more carefully and look for skewness.

\newpage

```{r classDistributions, fig.height=7.5, fig.cap="Collection of barplots for every variable, coloured by class variable."}
# Function that creates a barplot on dataset given a column name
barplots <- function(colName) {
  ggplot(dataset, aes(x = !!sym(colName), fill = !!sym(colName))) +
  geom_bar(show.legend = FALSE) + ylab("") +
  theme(text=element_text(size=8))
}

# Gather column names from entire dataset
cols <- colnames(dataset)
# execute barplots function on every column
plot_list <- lapply(cols, barplots)
# arrange these boxplots into one figure
ggarrange(plotlist = plot_list, ncol = 4, nrow = 4)
```

The first thing we notice is that a lot of the categorical variables are very skewed towards one particular value. In fact, all but two of these variables seem to have a significant level of skewness present. Removing most of our variables because of skewness would be foolish, so this is something we'll have to take into account when selecting an algorithm later on. One possible set of techniques to look into would be Synthetic Minority Oversampling Technique or SMOTE for short. What this entails and whether it is a good fit for our needs is something to be looked into later.\
Something worth noting about our numeric variables is that all of them look to be normally distributed when examined with the naked eye. This is not of immediate concern but certainly good to know.\
In a similar vain, one might notice a small detail that wasn't mentioned in our dataset's source. The age range for this dataset looks to be thirty to seventy year olds. Again, not something that is of immediate concern, but certainly good to know.

### Correlation

Many machine learning algorithms assume all attributes are independent. It is therefore important to check whether the variables in our dataset are actually independent, or if they turn out to be correlated. We'll do this by creating a correlation matrix. In this matrix the size and shade of the circle indicate the amount of correlation. A larger circle indicates a stronger correlation, so a smaller circle in turn indicates the opposite. A blue shade demonstrates a positive correlation while a red shade illustrates a negative correlation. Therefore the lighter shades in between indicate the lack of a correlation.

```{r correlation, fig.cap="Matrix of degree of correlation between variables."}
# Convert all columns in the dataset to numeric data
dataset.numeric <- sapply(dataset, as.integer)
# Get correlation metrics on the dataset
correlation <- cor(dataset.numeric)
# Create a correlation matrix using the correlation metrics
corrplot(correlation, type = "lower", tl.col = "black", tl.cex = 0.85, tl.offset = 1, 
         title = "Correlation matrix", outline = TRUE, mar = c(0, 0, 1, 0), cl.cex = 0.75)
```

A good control point indicating the validity of this correlation matrix can be found between diabetes and glucose. This correlation has been studied extensively to the point where it is essentially common knowledge that high glucose levels correlate with diabetes. Seeing this correlation in our own data is a nice confirmation that this plot is correct. Apart from that, the correlation between these two variables indicates it might be preferable to choose one of them over the other going forward.\
Let us now dive into the other correlations. One fairly obvious correlation is the one found between current smokers and the amount of cigarettes smoked per day. Since these variables are so similar and definitely not independent of each other, it might be worth dropping one of them depending on our choice of algorithm later.\
The same can be said for systolic blood pressure and diastolic blood pressure. Since the former measures the amount of force put on the arteries as the heart beats, and the latter measures the amount of force on the arteries when the heart is in a state of rest, they might as well be combined into a single blood pressure variable if the chosen algorithm requires it.\
Apart from correlating with each other the two variables named above correlate with hypertension as well. This too is a fairly obvious correlation since hypertension denotes the subject has high blood pressure. Another possible solution would therefore be to combine all three of these variables into a single feature if necessary.

### PCA - Principal Component Analysis

When trying to determine whether our data clusters together one might be inclined to create a scatter plot. This is a valid approach when trying to compare two different variables. Comparing three different variables would already be slightly more challenging since this would require a three dimensional plot. This would become physically impossible however when trying to compare four or more variables, since our mortal souls are limited to a mere three dimensional space. This is where Principle Component Analysis, or PCA for short, comes in. PCA is a technique where the dimensions (variables) are reduced by converting them into Principle Components. These PCs are ordered by their impact on the variation of the data, where PC1 has the highest impact. The first two PCs will be plotted against each other, where every variable has an arrow indicating it's impact on these first two PCs. Since the plotted PCs have the highest impact on the data's variance, and the biggest arrows have the largest impact on the PCs, larger arrows indicate more significant variables. Furthermore arrows pointing the same way indicate positive correlation, while opposing arrows indicate negative correlation.

```{r PCA, fig.cap="PCA plot charting PC2 against PC1."}
# Get PCA calculation on numeric dataset
pca <- prcomp(dataset.numeric, center = TRUE, scale. = TRUE)
# Plot PCA calculation
ggbiplot(pca, obs.scale = 1, var.scale = 1,
         groups = dataset$TenYearCHD, circle = TRUE, alpha = 0.35) +
  scale_color_discrete(name = "Risk of CHD") +
  ggtitle("Principal Component Analysis")
```

In the plot above we can clearly see a contrast between the location of subjects with and without ten year risk of CHD. The subjects with risk of CHD cluster further along the x-axis than subjects lacking this risk. Since PC1 (x-axis) accounts for the larger amount of variation in our data (21.6%) differences along the x-axis are to be considered more significant than differences along the y-axis.

## Exporting data

Now that we have analysed our data and cleaned it accordingly we can go ahead and export it. We'll export into two formats: Comma Separated Values (csv) for easy usage with reporting, and Attribute Relation File Format (ARFF) for usage in the Weka software package. We'll place both of these new data files in the data folder of this project's directory.

```{r exportData}
# To CSV
write.csv(dataset, file = "../data/processedData.csv")
# To ARFF
RWeka::write.arff(dataset, file = "../data/processedData.arff")
```

\newpage
# Machine Learning algorithm

Now that we have processed and exported our dataset we can take a big step forward in this project. It is finally time to look at how we shall apply methods of Machine Learning to our data. For easy, visual interaction with our data we will open the previously exported ARFF file in Weka. Once loaded we can start to do all kinds of fun things to our dataset. Before we just start clicking random buttons though, we should discuss our options first.

## Algorithm metrics

Instead of just haphazardly trying out different algorithms, it'd be a better idea to go about this process a bit more systematically. We'll start by listing some important metrics to consider when comparing algorithms and briefly discuss their impact regarding this particular dataset. After this we'll inventorise for which algorithms we'll collect these metrics.

### Quality & performance metrics

Weka provides us with loads of metrics which we can combine into performance graphs in whichever way we'd like. Taking in every single one of these measurements would be an overwhelming amount of information, so we'll pick the most important ones and discuss them below.

#### Speed

Speed is far from the most important metric when considering the context of our data. After all, this isn't some big social media website needing to fulfill millions of requests at any time of day. It is however still an interesting metric to remember since the difference between testing an instance in a few seconds versus half an hour is still fairly significant.

#### Accuracy

Quick? Yes. Reliable? No. Accuracy is simply a measure for which instances an algorithm classified correctly. Seems sensible right? It is right there, front and centre, when running any algorithm and it is easy to grasp. [Yet solutions that are the first thing you think of, seem sensible, and are easy to implement are often terrible, ineffective solutions, once implemented will drag on civilisation forever](https://youtu.be/oAHbLRjF0vo?t=31). And so it is for accuracy in the machine learning sphere. Accuracy is often still used as the be all and end all solution for grading an algorithms performance, especially by folk new to the field. It will be included here as a quick and dirty indication, but we will combine it with other, more informative solutions.

#### The confusion matrix

This matrix is important to understand since it's the key to understanding the metrics yet to be discussed. The confusion matrix does exactly as it's name suggests. It provides us with an overview of how "confused" our algorithm is, and communicates this in an easy to understand matrix.\
In this environment "confused" refers to the instances the algorithm in question labelled improperly. This can be in the form of instances incorrectly labelled as positive, known as the "false positives" (FP). Counter to that, we can have positive instances labelled as negative instead. We call these "false negatives" (FN). Correctly labelled records are intuitively referred to as "true positives" (TP) and "true negatives" (TN). The number of real positive and negative cases in the data are known as "condition positive" (P) and "condition negative" (N) respectively.

An example of a confusion matrix:

\begin{table}[h]
\caption{Example confusion matrix.}
\centering
  \begin{tabular}{ |l|l||l|l|  }
    \hline
    & \multicolumn{3}{c|}{\textbf{Predicted condition}} \tabularnewline
    \hline
    & & \textbf{Positive} & \textbf{Negative} \\
    \hline
    \hline
    \multirow{3}{*}{\rotatebox{90}{\textbf{Actual condition \hspace{1em}}}} & & & \\
    & & & \\
    & \textbf{Positive} & 52 (TP) & 8 (FN) \\ 
    & & & \\
    & & & \\
    \cline{2-4}
    & & & \\
    & & & \\
    & \textbf{Negative} & 4 (FP) & 32 (TN) \\
    & & & \\
    & & & \\
    \hline
  \end{tabular}
\label{table:confMatrix}
\end{table}

\newpage

#### F-measure

This metric is a score based on two other metrics, both of which are based on the confusion matrix discussed above. The two parts that make up the F-measure are the precision (also known as positive predictive value (PPV)) and the recall (also known as sensitivity, hit rate, or true positive rate (TPR)). The former represents the fraction of correctly labelled instances out of all instances labelled as positive. In formulaic form it would be described by $PPV = \frac{TP}{TP + FP}$. The latter of our two parts describes the fraction of positive instances which were labelled as so. It's formula turns out as follows: $TPR = \frac{TP}{P} = \frac{TP}{TP +FN}$. \
Both of these values can then be combined into the F-measure with the following formula: 
$$F = 2 \times \frac{PPV \times TPR}{PPV + TPR} = \frac{2TP}{2TP + FP + FN}$$ 
It represents the harmonic mean of precision and recall, where an outcome of $1$ would indicate both of these values are perfect and an outcome of $0$ would indicate both are zero as well.

#### ROC Area

Last but not least is the Receiver Operating Characteristic Area. This name may seem a little odd, but that's because electrical engineers and radar engineers came up with it. Like with the F-measure, the ROC Area is another metric that's made up of two other measures, those being the true positive rate (TPR) (also referred to as benefit) and the false positive rate (FPR) (also referred to as cost). These respectively describe the instances correctly and incorrectly labelled as positive. When we plot the cumulative distribution function of these two metrics against each other, with TRP on the y-axis and FPR on the y-axis, we may calculate the area under the curve. An area of $1$ corresponds to a most optimal model, an area of exactly $0.5$ corresponds to as good as a random guess, and an area below $0.5$ corresponds to worse than a random guess. \
The latter of these three options seems like an abominable outcome, but it may not be so. If we can *reliably* predict the *wrong* answer, we can simply flip it around to reliably predict the *right* answer. This method will work either way, as long as we're sure enough about the answer's correctness.

#### Metrics overview

Now that we know what to look for in an algorithm we can summarise this in a short overview.

\begin{table}[h]
\caption{Overview of algorithm scoring metrics.}
\centering
  \begin{tabular}{ |l||l|l|  }
    \hline
    \textbf{Metric} & \textbf{Description} & \textbf{Desired outcome} \\
    \hline
    \hline
    \textbf{\textit{Speed}} & How many seconds it takes to test the model & Lower is better \\ 
    \hline
    \textbf{\textit{Accuracy}} & Fraction of correctly labelled instances & Higher is better \\
    \hline
    \textbf{\textit{False Negative Rate}} & Fraction of incorrectly labelled positive instances & Lower is better \\
    \hline
    \textbf{\textit{F-measure}} & Harmonic mean of precision and recall & Higher is better \\
    \hline
    \textbf{\textit{ROC Area}} & Area under ROC curve where 0.5 equals a random guess & Deviation from 0.5 is better \\
    \hline
  \end{tabular}
\label{table:metricsOverview}
\end{table}

### Algorithm exploration

We know what to test for, but what do we actually want to test? Well, machine learning algorithms of course, but which ones? We'll pick a few algorithms from each of the main categories: \
As a start we'll pick ZeroR as a baseline, and then add OneR as a slightly more complex addition in the rules division. \
In the Trees section we'll pick J48 since it can handle both numeric and nominal data and random forest will be generated as well. \
In the lazy learning department we'll go with IBk, otherwise known as k-nearest neighbours. \
From Bayes we shall add Naive Bayes to our list of algorithms. \
Last but not least from the Rules category we'll pick Simple Logistic and Sequential Minimal Optimisation (SMO). \
All tables containing test results were generated by the Weka experimenter after which they were (partially) combined and edited stylistically. A white dot represents a statistically significant higher result than ZeroR, while a black dot represents a statistically significant lower result than ZeroR.

#### Default settings testing
To start we'll run all of these algorithms with their default settings, using ZeroR as the baseline, and performing 10-fold cross validation on the dataset.

\begin{table}[thb]
\caption{\label{DefaultMLSettings}References to ML algorithms and their (default) settings.}
\scriptsize
{\centering
\begin{tabular}{cl}\\
(1) & rules.ZeroR '' \\
(2) & rules.OneR '-B 6' \\
(3) & trees.J48 '-C 0.25 -M 2' \\
(4) & trees.RandomForest '-P 100 -I 100 -num-slots 1 -K 0 -M 1.0 -V 0.001 -S 1' \\
(5) & lazy.IBk '-K 1 -W 0 -A \textbackslash"weka.core.neighboursearch.LinearNNSearch -A \textbackslash\textbackslash\textbackslash"weka.core.EuclideanDistance -R first-last\textbackslash\textbackslash\textbackslash \\
(6) & bayes.NaiveBayes '' \\
(7) & functions.SimpleLogistic '-I 0 -M 500 -H 50 -W 0.0' \\
(8) & functions.SMO '-C 1.0 -L 0.001 -P 1.0E-12 -N 0 -V -1 -W 1 -K \textbackslash"functions.supportVector.PolyKernel -E 1.0 -C 250007\textbackslash" -calibrator \textbackslash"functions.Logistic -R 1.0E-8 -M -1 -num-decimal-places 4\textbackslash"'  \\
\end{tabular}
}
\end{table}


\begin{table}[thb]
\caption{\label{DefaultComparison}Comparison of ML algorithms using default settings using ZeroR as a baseline.}
\footnotesize
{\centering \begin{tabular}{lr||r@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}c}\\
\hline
\textbf{Algorithm} & \textit{ZeroR} & \textit{OneR} & & \textit{J48} & & \textit{RandomForest} & & \textit{IBk} & & \textit{NaiveBayes} & & \textit{SimpleLogistic} & & \textit{SMO} & \\
\hline
\hline
\textbf{Elapsed time testing} & 0.00 & 0.00 &         & 0.00 &         & 0.01 & $\circ$ & 0.04 & $\circ$ & 0.00 & $\circ$ & 0.00 &         & 0.00 &        \\
\hline
\textbf{Accuracy} & 84.80 & 84.41 & $\bullet$ & 83.98 & $\bullet$ & 84.92 &         & 78.52 & $\bullet$ & 81.79 & $\bullet$ & 85.43 & $\circ$ & 84.79 &        \\
\hline
\textbf{False Negative Rate} & 0.00 & 0.01 & $\circ$ & 0.03 & $\circ$ & 0.01 & $\circ$ & 0.12 & $\circ$ & 0.08 & $\circ$ & 0.01 & $\circ$ & 0.00 &        \\
\hline
\textbf{F-measure} & 0.92 & 0.92 & $\bullet$ & 0.91 & $\bullet$ & 0.92 &         & 0.87 & $\bullet$ & 0.90 & $\bullet$ & 0.92 & $\circ$ & 0.92 &        \\
\hline
\textbf{ROC Area} & 0.50 & 0.51 & $\circ$ & 0.64 & $\circ$ & 0.69 & $\circ$ & 0.56 & $\circ$ & 0.71 & $\circ$ & 0.73 & $\circ$ & 0.50 &        \\
\hline
\multicolumn{16}{c}{$\circ$, $\bullet$ statistically significant increase or decrease}\\
\end{tabular} \footnotesize \par}
\end{table}

The thing to notice in these results is that with both the Accuracy as well as the F-measure metric, we get performances which are statistically significantly worse than ZeroR, our baseline algorithm. Even though the ROC Area looks fine, this result isn't any good. This occurance can be explained by our skewed data from the chapter on \nameref{1.2.3}, in particular our skewed class variable. Since 84.8% of our TenYearCHD variable is classified as `no` even ZeroR will get an accuracy of 84.8%, even though this algorithm isn't any good. We will fix this issue with the method we mentioned before; SMOTE. 

#### SMOTE

With this technique we will artificially create extra instances of the `yes` kind of our class variable based on other values in the dataset. Luckily we can simply do this in Weka. First we open our existing dataset in the Weka explorer after which we will select the `SMOTE` filter under `filters > supervised > instance`. We then tune the parameters as follows: \
`classValue` = 0 \
`nearestNeighbors` = 5 \
`percentage` = 450 \
This means we select the right instances to increase, base new instances on it's 5 nearest neighbors, and increase the number of instances by 450% therefore balancing the `yes` and `no` labels. \
We have now added enough synthetic instances to balance our classes, but SMOTE introduces a new problem. When creating new instances it will produce values with a tremendous amount of decimal places. If we were to just use these new values as is, we'd get a huge bias towards these synthetic instances, in the process probably turning OneR into the most ideal algorithm. To remedy this we will round the values in our new post-SMOTE dataset to the same number of decimals as the pre-SMOTE dataset. To do this we will apply another filter: `filters > unsupervised > attribute > NumericCleaner`, which we will apply using default settings except for the `decimals` parameter. We will set this to whichever number of decimals the attribute initially had. Once we have done this we can then save this dataset and return to the Weka experimenter. After completing all these steps we get the following results:

\begin{table}[thb]
\caption{\label{DefaultSMOTEComparison}Comparison of ML algorithms using default settings after performing SMOTE on the dataset, using ZeroR as a baseline.}
\footnotesize
{\centering \begin{tabular}{lr||r@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}c}\\
\hline
\textbf{Algorithm} & \textit{ZeroR} & \textit{OneR} & & \textit{J48} & & \textit{RandomForest} & & \textit{IBk} & & \textit{NaiveBayes} & & \textit{SimpleLogistic} & & \textit{SMO} & \\
\hline
\hline
\textbf{Elapsed time testing} & 0.00 & 0.00 &         & 0.00 &         & 0.03 & $\circ$ & 0.12 & $\circ$ & 0.00 & $\circ$ & 0.00 &         & 0.00 &        \\
\hline
\textbf{Accuracy} & 50.36 & 80.28 & $\circ$ & 74.83 & $\circ$ & 84.23 & $\circ$ & 77.06 & $\circ$ & 62.33 & $\circ$ & 67.36 & $\circ$ & 67.17 & $\circ$\\
\hline
\textbf{False Negative Rate} & 0.00 & 0.09 & $\circ$ & 0.26 & $\circ$ & 0.17 & $\circ$ & 0.31 & $\circ$ & 0.22 & $\circ$ & 0.33 & $\circ$ & 0.36 & $\circ$\\
\hline
\textbf{F-measure} & 0.67 & 0.82 & $\circ$ & 0.75 & $\circ$ & 0.84 & $\circ$ & 0.75 & $\circ$ & 0.67 &         & 0.67 &         & 0.66 &        \\
\hline
\textbf{ROC Area} & 0.50 & 0.80 & $\circ$ & 0.77 & $\circ$ & 0.92 & $\circ$ & 0.77 & $\circ$ & 0.70 & $\circ$ & 0.73 & $\circ$ & 0.67 & $\circ$\\
\hline
\multicolumn{16}{c}{$\circ$, $\bullet$ statistically significant increase or decrease}\\
\end{tabular} \footnotesize \par}
\end{table}

We now see that ZeroR doesn't perform nearly as well as it did before, and most of the other algorithms are already significantly better. This is a much better result and more accurately reflects what initial results should look like. \
Now that we have some decent results we can take a look at which algorithms actually perform well. When taking each metric into account we see that both the J48 tree as well as RandomForest perform quite well, but they are not quite perfect. They have a false negative rate of 26% and 17% respectively. Although this is quite a bit lower than most of the other algorithms, it is still quite dubious to use an algorithm which classifies $\frac{1}{10}$ of positive instances as negative.

#### Cost Sensitive Classifier

To remedy the issue of false negatives we can apply a technique called cost sensitive classification. This classifier allows one to apply a custom amount of cost to values in the confusion matrix. In our case false negatives are a great sin, so we'll try doubling its cost two as compared to false positive values' cost of one. We can do this in Weka by wrapping all our algorithm choices in a cost sensitive classifier: Instead of immediately choosing an algorithm we'll go `meta > CostSensitiveClassifier` and then enter an algorithm of choice at the `classifier` option. We can then proceed to enter our 2x2 `costMatrix` and set `minimizeExpectedCost` to `True`. After doing this for each algorithm we get the following results.

\begin{table}[thb]
\caption{\label{DefaultSMOTEComparisonCSC}Comparison of ML algorithms using default settings and cost sensitive classification after performing SMOTE on the dataset, using ZeroR as a baseline.}
\footnotesize
{\centering \begin{tabular}{lr||r@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}c}\\
\hline
\textbf{Algorithm} & \textit{ZeroR} & \textit{OneR} & & \textit{J48} & & \textit{RandomForest} & & \textit{IBk} & & \textit{NaiveBayes} & & \textit{SimpleLogistic} & & \textit{SMO} & \\
\hline
\hline
\textbf{Elapsed time testing} & 0.00 & 0.00 &         & 0.00 &         & 0.03 & $\circ$ & 0.12 & $\circ$ & 0.00 & $\circ$ & 0.00 &         & 0.00 &        \\
\hline
\textbf{Accuracy} & 50.36 & 80.28 & $\circ$ & 74.65 & $\circ$ & 80.29 & $\circ$ & 77.06 & $\circ$ & 61.80 & $\circ$ & 61.95 & $\circ$ & 67.17 & $\circ$\\
\hline
\textbf{False Negative Rate} & 0.00 & 0.09 & $\circ$ & 0.23 & $\circ$ & 0.04 & $\circ$ & 0.31 & $\circ$ & 0.15 & $\circ$ & 0.12 & $\circ$ & 0.36 & $\circ$\\
\hline
\textbf{F-measure} & 0.67 & 0.82 & $\circ$ & 0.75 & $\circ$ & 0.83 & $\circ$ & 0.75 & $\circ$ & 0.69 & $\circ$ & 0.70 & $\circ$ & 0.66 &        \\
\hline
\textbf{ROC Area} & 0.50 & 0.80 & $\circ$ & 0.75 & $\circ$ & 0.80 & $\circ$ & 0.77 & $\circ$ & 0.62 & $\circ$ & 0.62 & $\circ$ & 0.67 & $\circ$\\
\hline
\multicolumn{16}{c}{$\circ$, $\bullet$ statistically significant increase or decrease}\\
\end{tabular} \footnotesize \par}
\end{table}

In these results we can see that some of our best performing algorithms, including our best: the RandomForest, take a hit in accuracy and ROC Area. However, we do get a significantly lower false negative rate in return for that hit. The two best performing algorithms that we are left with look to be OneR and RandomForest. We'll continue our analysis with just these two, since optimising every algorithms would be too time intensive for this project.

### Algorithm optimisation

Now that we have found our best performing algorithms, we can continue on to optimising their settings as much as possible. To do this we'll once again use the Weka experimenter. In contrast to the last chapter we will stop using ZeroR as a baseline at this point. Instead we'll be using the default settings setup of each algorithm as a baseline for optimising that particular algorithm. 

#### OneR

For our convenience OneR only has a single setting that really interests us, which is its minimum bucket size.

##### Minimal bucket size

In Weka the default value for this setting is six which we will therefore use as a baseline in this analysis. Since we have a fairly large dataset we can afford to increase this value dramatically as we do not want to end up with a nonsensical model. When OneR selects its preferred attribute it will start to classify instances based on that attribute. When the bucket size is too low it might create some weird models. For example: \
Let's say OneR picked, for instance, age as the attribute it wants to use. When the bucket size is too low it might classify an instance with an age of 38 as `yes`, an instance with an age of 39 as `no`, and an instance with an age of 40 as `yes` again. This is a bit odd, and to prevent this we will increase the bucket size. \

\begin{table}[thb]
\caption{\label{OneRBucketSize}Comparison of ZeroR using varying bucket sizes and cost sensitive classification after performing SMOTE on the dataset, using ZeroR's default settings as a baseline.}
\footnotesize
{\centering \begin{tabular}{lr||r@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}c}
\hline
\textbf{OneR setting} & \textit{-B 6} & \textit{-B 25} & & \textit{-B 50} & & \textit{-B 100} & & \textit{-B 200} &  \\
\hline
\hline
\textbf{Elapsed time testing} & 0.00 & 0.00 &         & 0.00 &         & 0.00 &         & 0.00 &        \\
\hline
\textbf{Accuracy} & 80.28 & 74.19 & $\bullet$ & 64.39 & $\bullet$ & 64.89 & $\bullet$ & 64.90 & $\bullet$\\
\hline
\textbf{False Negative Rate} & 0.09 & 0.23 & $\circ$ & 0.43 & $\circ$ & 0.47 & $\circ$ & 0.46 & $\circ$\\
\hline
\textbf{F-measure} & 0.82 & 0.75 & $\bullet$ & 0.62 & $\bullet$ & 0.61 & $\bullet$ & 0.61 & $\bullet$\\
\hline
\textbf{ROC Area} & 0.80 & 0.74 & $\bullet$ & 0.64 & $\bullet$ & 0.65 & $\bullet$ & 0.65 & $\bullet$\\
\hline
\multicolumn{10}{c}{$\circ$, $\bullet$ statistically significant increase or decrease}\\
\end{tabular} \footnotesize \par}
\end{table}

Unsurprisingly, as we make our model more sane by increasing the bucket size, the model's performance drops significantly in every single measure, except of course for elapsed time. This is a very clear indication that OneR is probably not the algorithm we'd like to continue on with. Luckily we spread our eggs over two baskets, so let us now take a look at the Random Forest.

#### RandomForest

This algorithm has a bunch more settings than OneR, as it is a bit more complicated. There is however only one setting that we are really interested in here, which is `numIterations`.

##### Number of iterations
The number of iterations we use when producing our forest represents the number of trees in our forest. Theoretically we should get a better result with more iterations since producing more trees increases the chance we'll find a better one.

\begin{table}[thb]
\caption{\label{RandomForestIterations}Comparison of RandomForest using a varying number of iterations and cost sensitive classification after performing SMOTE on the dataset, using RandomForest's default settings as a baseline.}
\footnotesize
{\centering \begin{tabular}{lr||r@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}c}
\hline
\textbf{RandomForest setting} & \textit{-I 100} & \textit{-I 250} & & \textit{-I 500} & & \textit{-I 750} & & \textit{-I 1000} &  \\
\hline
\hline
\textbf{Elapsed time testing} & 0.03 & 0.14 & $\circ$ & 0.36 & $\circ$ & 8.17 &         & 0.89 & $\circ$\\
\hline
\textbf{Accuracy} & 80.29 & 80.51 &         & 80.53 &         & 80.51 &         & 80.55 &        \\
\hline
\textbf{False Negative Rate} & 0.04 & 0.04 &         & 0.04 &         & 0.04 &         & 0.04 &        \\
\hline
\textbf{F-measure} & 0.83 & 0.83 &         & 0.83 &         & 0.83 &         & 0.83 &        \\
\hline
\textbf{ROC Area} & 0.80 & 0.80 &         & 0.80 &         & 0.80 &         & 0.80 &        \\
\hline
\multicolumn{10}{c}{$\circ$, $\bullet$ statistically significant increase or decrease}\\
\end{tabular} \footnotesize \par}
\end{table}

Although we do see some changes in our accuracy values, none of them are statistically significant. We do however see significant changes in our elapsed time testing, but these increases are actually a negative consequence. All this means that we can stick with a Random Forest doing 100 iterations.

### Meta-learners

## Java wrapper
