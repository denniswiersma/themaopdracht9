---
title: "Exploratory Data Analysis"
author: "Dennis Wiersma"
date: "2022-09-14"
output: pdf_document
urlcolor: blue
---

```{r, include=FALSE}
# Copyright (c) 2022 Dennis Wiersma.
# Licensed under GPLv3. See LICENSE file.
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries, include=FALSE}
# Loading required libraries
```

# Introduction
Cardiovascular disease gets more prominent by the day. As one of the leading causes of death in the developed world it is an important issue to examine. Getting to know more about this matter and what factors impact risk of complications can therefore be rather interesting. This document describes the Exploratory Data Analysis which functions as a kick off point for this project.

# Dataset
The dataset used in this research project contains data from an ongoing cardiovascular study on residents of the town of Framingham, Massachusetts, United States of America. This study is better known as the [Framingham Heart Study](https://www.framinghamheartstudy.org/) (FHS). 

## Loading the data
The subset of this large collection of data used for this research contains over 4000 records and 15 different variables. In this section all necessary files will be loaded for analysis.

### Loading the codebook
The codebook is a document describing variables and listing additional information like units and datatypes.  
The codebook will be loaded as follows:
```{r loadingCodebook}
# Load codebook
codeBook <- read.table(file = "../CodeBook.txt", header = TRUE, sep = "|")
# Create table w/ codebook contents
knitr::kable(codeBook, "simple")
```

### Loading the dataset
To analyse our dataset we'll of course need access to it. It will be loaded with the code below:
```{r loadingDataset}
# Load dataset
dataset <- read.table(file = "../data/framingham.csv", header = TRUE, sep = ",")
# Get dimensions of dataset
dataset.dim <- dim(dataset)
# Print dimensions of dataset
cat("Instances: ", dataset.dim[1], "\nvariables: ", dataset.dim[2])
```

### Dataset inconsistencies
The dataset mentioned above was found on the Kaggle website. There are a few problems with this data. First off there is the "education" variable. This variable consists of values 1 through 4, presumably to indicate education levels. However, what these numbers represent is not explained anywhere in the dataset's description. Thus far it is still unclear what exactly these values specify.  
An example of this education variable:
```{r educationExample}
# Get head of education column and print table
knitr::kable(head(dataset["education"]), "simple")
```

Also included in this dataset are some numeric variables, described on the dataset's page as "continuous". These values all lack a description of their units. Where a variable's unit can sometimes easily be interpreted, like with heart rate, other times it means we end up with no unit whatsoever.  
The following variables have missing units:
```{r missingUnitList}
# Print codebook entries where unit is NA
knitr::kable(codeBook[which(is.na(codeBook["unit"])), c("name", "unit")])
```

To resolve these missing units one may look for the source of the data. The source for this data mentioned on the Kaggle page links to another page on Kaggle. The problem is that this page no longer exists. We do however know that this data originates from the [Framingham Heart Study](https://www.framinghamheartstudy.org/) (FHS). Searching through this website for our missing units led to [this page](https://www.framinghamheartstudy.org/fhs-for-researchers/data-available-overview/) about available FHS data. From there we find a page on [FHS Dataset Inventory for Public Viewing](https://wwwapp.bumc.bu.edu/FHSresapp/Public/DataSetList) which lists all datasets which can be requested from the FHS team. This page includes documentation on variables. This sounds like progress but there are fifty pages of datasets to choose from. Finding out which of these datasets was used to source the data used in this project would be too time intensive. Therefore, there is no other choice but to work with what we have and use the data without it's corresponding units.
