---
title: "Predicting risk of Coronary Heart Disease using Machine Learning"
author: "Dennis Wiersma"
date: "`r Sys.Date()`"
urlcolor: blue
output: 
  pdf_document: 
    highlight: kate
    number_sections: yes
    citation_package: natbib
    
    includes: 
      in_header: preamble.tex
      before_body: prefix.tex
      
bibliography: references.bib
---

```{r, include=FALSE}
# Copyright (c) 2022 Dennis Wiersma.
# Licensed under GPLv3. See LICENSE file.
```

```{r setup, include=FALSE}
# Excludes all code from the report
knitr::opts_chunk$set(echo = FALSE)
```

```{r libraries, include=FALSE}
# Loading required libraries
library("ggplot2")
library("ggpubr")
library("ggbiplot")
library("corrplot")
```

```{r loadingDataset}
# Load datasets
dataset <- read.table(file = "../data/processedData.csv", header = TRUE, sep = ",", stringsAsFactors = TRUE)
postSMOTE_dataset <- read.table(file = "../data/roundedSMOTEprocessedData.csv", header = TRUE, sep = ",", stringsAsFactors = TRUE)
```

\newpage

# Introduction

Heart disease is a condition that gets more prevalent by the day. It is one of the most, if not the most, occurring cause of death in this day and age [@World_Health_Organization_2020_who]. It is therefore important to know what factors can lead to someone developing heart disease. Thankfully, a lot is already known about what goes into developing this condition. So, if it is known what causes heart disease, then what can we do with this information? The answer is prevention. Possibly just as important as knowing what causes heart disease would be to know whether someone is likely to develop a heart condition in the near future. After all, if one knows they're at risk of succumbing to the number one killer in the world, they can still make an effort to subvert disaster. In the process of gathering such information a patient could seek advice from their physician, who could then evaluate their lifestyle and physical condition. Using this information the medical practitioner can then estimate whether the patient is likely to be subject to the unease of heart disease.\
Computer scientists wouldn't be computer scientists if they weren't trying to program someone out of a job, so what if this evaluation by a doctor could be automated by a computer. A piece of software could evaluate data from dozens of patients in mere seconds. This would cause the need for a physician to greatly decrease for this procedure, lowering both cost and barrier to entry.\
To achieve this dream of automation a range of techniques were used, the most important of which is Machine Learning (ML) which is a sub field of Artificial Intelligence (AI). Using these ML techniques a model can be trained using a large amount of data. This model can then predict certain values for a given input. For example, such a model could predict whether a patient has the risk of developing heart disease when given information about that patient's medical condition.\
As mentioned before, for the training of this ML model a rather large amount of data is required. The dataset which was used for completing this training phase was gathered from the Framingham Heart Study (FHS) and contains information on the risk of subjects developing Coronary Heart Disease (CHD) within the next ten years. Since these models can be fairly specific to the data on which they have been trained, any bias in the dataset will also translate into the model produced from it. This therefore leaves us with the following question to answer: How well can one predict 10 year risk of coronary heart disease in subjects from the Framingham Heart Study?

\newpage

# Methodology

For the successful execution of this project a collection of techniques and software were used instructions for which were provided by Hanze University of Applied Sciences [@Michiel_Noback_2022_hanze]. This, together with the process of going from raw data to the classification of brand new data, will be discussed in this chapter.

## Software

An overview of software, packages, and programming languages used for this project. The rest of this chapter will unfold when and where these various tools were used.

```{=tex}
\begin{table}[h]
\caption{Software used for this project.}
\begin{tabular}{l||rl}
\hline
Software & Version  & Function  \\ 
\hline
\hline
\textbf{R} & 4.2.1 & Statistical programming \\
\hline
\textbf{Java} & 17.0.5 & General purpose programming \\
\hline
\textbf{Weka} & 3.8.6 & Machine learning software workbench \\
\hline
\end{tabular}
\end{table}
```
```{=tex}
\begin{table}[h]
\caption{R packages used for this project.}
\begin{tabular}{l||rl}
\hline
R package & Version  & Function  \\ 
\hline
\hline
\textbf{knitr} & 1.40 & Dynamic report generation \\
\hline
\textbf{ggplot2} & 3.3.6 & Data visualisation \\
\hline
\textbf{ggpubr} & 0.4.0 & Arranging plots \\
\hline
\textbf{ggbiplot} & 0.55 & PCA plot \\
\hline
\textbf{corrplot} & 0.92 & Correlation plot \\
\hline
\textbf{RWeka} & 0.4-44 & Writing data to ARFF \\
\hline
\end{tabular}
\end{table}
```
```{=tex}
\begin{table}[h]
\caption{Java packages used for this project.}
\begin{tabular}{l||rl}
\hline
Java package & Version  & Function  \\ 
\hline
\hline
\textbf{Gradle} & 7.4 & Project management \\
\hline
\textbf{Weka API} & 3.8.0 & Weka programming interface \\
\hline
\textbf{ShadowJar} & 7.1.2 & Fat JAR building \\
\hline
\end{tabular}
\end{table}
```
## Data

As mentioned before, training a machine learning algorithms requires data. This data is a subset of the statistics gathered for the FHS and contains over four thousand instances and sixteen variables.

### Acquisition

Obtaining the data was a fairly easy process of hunting down an interesting dataset on the [Kaggle website](https://www.kaggle.com/) that fulfilled a certain set of requirements. These requirements include, but are not limited to:

* Choosing a supervised learning problem. This means the dataset in question is to contain class labels.
* The set contains data from the life sciences. This includes, for example, biology, chemistry, and medicine.
* At least seven attributes are present in the dataset, but preferably more.
* At least several hundred instances are present in the dataset, but preferably more.

At the close of hunting season a Kaggle page called "Logistic regression To predict heart disease" [@Dileep_2019_kaggle] was chosen for this project. It is the page that contains the subset of data from the FHS. One thing to note about this Kaggle page is that nowhere does it mention how they acquired this data from the FHS project. It simply links to another, by now defunct, Kaggle page. Apart from that it just has a download link to the data, some information, and a not that comprehensive analysis.

### Processing

After acquiring the dataset, the first thing is of course to load it into one's software package of choice, which in this case would be R. Once the data has been loaded it is time to take a good look at it, and it is at this point that the first problem arises. This collection of data contains an `education` attribute consisting of values ranging from one through to four. An explanation of what these numbers represent is nowhere to be found. It was therefore decided to drop this variable, since it is not entirely clear what exactly it represents.

Moving on one runs into the second issue, which are the missing units. Just like a description of the `education` characteristic was missing, a lot of units for the given measurements were missing. Reading the Kaggle page and scouring the FHS website yielded no results, but luckily some of the missing units were easily interpreted.

Now that the missing metadata has been dealt with it was time to correctly label the data. Some of the data consisted of binary nominal values but were encoded in numerics. These variables were transformed into R's factor objects. These factors consist of levels like `male` and `female`, or `yes` and `no`. These are perfect for encoding nominal attributes.

While the missing metadata and wrongly encoded data have been resolved, the missing values in our dataset have not been taken care of yet. Resolving these so called `NA` values will be done by reviewing them on an attribute by attribute basis. \
First up are the amount of cigarettes smoked per day by a given subject. The initial step was to check whether every instance that had a missing value to their name was currently a smoker. This turned out to be the case, which meant these values could be imputed from the remaining values. This imputation was done by taking the mean number of cigarettes smoked \textit{by subjects that are currently smokers} and assigning this mean value to the gaps in our data. \
Next were the missing values for whether the subject was or wasn't on blood pressure medication. After looking at the dataset it was determined that, as one might suspect, a very large majority of subjects were in fact not on blood pressure medication. It was therefore assumed that the instances with the missing values would not be on these types of medication either. \
The following attributes were dealt with in bulk since the process of imputation was precisely identical for each of them. The variables in question are:

* Total cholesterol levels
* BMI
* Heart rate
* Glucose levels

Since all of these attributes consist of plain integer data they can easily be imputed. It was decided that the imputation process would simply consist of assigning the missing values the respective attribute's mean values.

Last but not least there's the problem regarding our so called class variable. This is the attribute which tells us whether a subject actually does run the risk of developing CHD in the next ten years. This attribute turned out to be heavily skewed towards subjects without said risk. This could potentially pose a myriad of issues during the training of our ML model. This issue was solved using Synthetic Minority Oversampling Technique (SMOTE). This is an approach where new (synthetic) instances are created which are labelled with the minority class, which would be the `yes` value in this case. The instances with this label were increased by 450% to solve the issue of imbalance. 
\newpage

## Machine Learning

Now that the data was ready, it was finally time to start work on the ML model. A lot of the work of exploring different algorithms, comparing different algorithms, comparing different hyper parameters, and figuring out what worked best was done using the Weka ML software workbench. 

### Performance

When trying to develop an ML model there are hordes of different algorithms to choose from, each of which perform better when applied in their own specific use case. So then, how does one figure out which algorithm works best for the data at hand? This is where the Weka experimenter comes in. This section of the Weka workbench lets users easily test and compare algorithms and parameters. \
Apart from finding a software solution, another question to answer is which scoring methods to use when making such comparisons. In the case of this research project this encompasses the following scoring methods:

* Classification speed
* Accuracy
* False Negative Rate (FNR)
* F-measure
* Area Under the Curve of Receiver Operating Characteristic (AUC-ROC)

These metrics will be used to compare different algorithms, but which algorithms to compare? ML algorithms can be divided into different categories. A few examples of these categories would be rule based, tree based, and lazy learning based algorithms. A selection of algorithms to compare has been carefully curated by ensuring every category of was represented. This resulted in the following algorithms being compared:

* ZeroR
* OneR
* J48
* Random Forest
* IBk
* Naive Bayes
* Simple Logistic
* Sequential Minimal Optimization (SMO)

The results of this comparison prompted the application of Cost Sensitive Classification (CSC). This is a technique where a cost can be applied to instances which are classified a certain way. The classifications that CSC can be applied to are True Positive (TP), True Negative (TN), False Positive (FP), and False Negative (FN) designations. In the context of this research a FN allotment regards a subject that does run the risk of developing CHD but would not be classified as such. It would be best to minimise this outcome, even if it means classifying a number of negative instances as positive. The final cost matrix can be found below:

\begin{table}[h]
\caption{Cost matrix where FN instances are punished more.}
  \begin{tabular}{ |c|c||c|c|  }
    \hline
    & \multicolumn{3}{c|}{\textbf{Predicted condition}} \tabularnewline
    \hline
    & & \textbf{Positive} & \textbf{Negative} \\
    \hline
    \hline
    \multirow{3}{*}{\rotatebox{90}{\textbf{Actual condition \hspace{1em}}}} & & & \\
    & & & \\
    & \textbf{Positive} & 0 (TP) & 2.5 (FN) \\ 
    & & & \\
    & & & \\
    \cline{2-4}
    & & & \\
    & & & \\
    & \textbf{Negative} & 1 (FP) & 0 (TN) \\
    & & & \\
    & & & \\
    \hline
  \end{tabular}
\label{table:confMatrix}
\end{table}

\newpage

When this initial comparison was concluded, the two best performing algorithms were chosen for optimisation. During this process, each algorithm's hyper parameters were revised such as to squeeze as much performance out of them as possible. The algorithm that performed best after this rigorous process was chosen as the algorithm of choice for the rest of this project. That, however, does not mean this will be the final model. There is still more performance to be gained. \
The last step before arriving at the final working model is the testing of so called meta classifiers. For this phase of testing both Bagging and Boosting techniques were applied to best performing algorithm thus far with the intention of creating an even better performing model. This concludes the final step of the ML model construction process. 

### Interfacing

A working ML model which can classify new instances as being or not being at risk of CHD is nice, but how does one actually go about interacting with this model to achieve such classification? This is another point where Weka comes into play. Apart from it's GUI workbench, Weka also provides an API for building models and classifying new, unknown instances. This API can be interacted with using the Java programming language. Using these tools a wrapper for our ML model was build in Java using Gradle for project management and the Weka API for classifying new instances based on our existing model. All this functionality has been neatly packed into a single JAR file which can be run using one's CLI of choice, as long as a Java install is present.

\newpage

# Results

The methodology described above generated a plethora of results, which will be showcased in this chapter.

## Data acquisition & preprocessing

During the data acquisition and preprocessing phase the data has been made more insightful using a series of visualisations. A number of these visualisations have been described below.

### Correlation

Different variables within a dataset may be correlated with each other. This could potentially pose problems for some ML algorithms since these may simply assume that all attributes are independent from each other. It is therefore important to be aware of whether there are correlations present within the dataset used in this project.

```{r correlation, fig.cap="Matrix of degree of correlation between variables."}
# Convert all columns in the dataset to numeric data
dataset.numeric <- sapply(dataset, as.integer)
# Get correlation metrics on the dataset
correlation <- cor(dataset.numeric)
# Create a correlation matrix using the correlation metrics
corrplot(correlation, 
         type = "lower", 
         tl.col = "black", 
         tl.cex = 0.85, 
         tl.offset = 1, 
         title = "Correlation matrix", 
         outline = TRUE, 
         mar = c(0, 0, 1, 0), 
         cl.cex = 0.75)
```

In this correlation matrix the size of each circle indicates the strength of a given correlation. When the circle is coloured a darker shade of blue it indicates a positive correlation, while a dark shade of red illustrates a negative correlation. Therefore the lighter shades of colour in between demonstrate the lack of a correlation.

When looking at the correlation matrix a few data points immediately jump out. The first one to look at is the strong positive correlation between glucose and diabetes. This correlation has been studied extensively to the point where it is essentially common knowledge that high glucose levels correlate with diabetes. Seeing this correlation within this dataset as well serves as a nice affirmation on the quality of the data. \
Another fairly obvious correlation can be found between whether a certain subject is a smoker and the number of cigarettes one smokes per day. These two attributes serve a very similar purpose, which is something to keep in mind during the ML training process. \
A very similar situation to the one just described has arisen between systolic and diastolic blood pressure. Since the former refers to the amount of force put on the arteries as the heart beats and the latter to the amount of force on the arteries when the heart is in a state of rest, it might be viable to merge these two attributes if the situation requires it. \
Apart from correlating with each other the two variables named above correlate with hypertension as well. This too is a fairly obvious correlation since hypertension denotes the subject has high blood pressure.

### Principal Component Analysis


When trying to determine whether our data clusters together one might be inclined to create a scatter plot. This is a valid approach when trying to compare two different variables. Comparing three different variables would already be slightly more challenging since this would require a three dimensional plot. This would become physically impossible however when trying to compare four or more variables, since our mortal souls are limited to a mere three dimensional space. This is where Principle Component Analysis (PCA) comes in. PCA is a technique where the dimensions (variables) are reduced by converting them into Principle Components. These PCs are ordered by their impact on the variation of the data, where PC1 has the highest impact. The first two PCs will be plotted against each other, where every variable has an arrow indicating it’s impact on these first two PCs. Since the plotted PCs have the highest impact on the data’s variance, and the biggest arrows have the largest impact on the PCs, larger arrows indicate more significant variables. Furthermore arrows pointing the same way indicate positive correlation, while opposing arrows indicate negative correlation.

```{r PCA, fig.cap="PCA plot charting PC2 against PC1."}
# Get PCA calculation on numeric dataset
pca <- prcomp(dataset.numeric, 
              center = TRUE, 
              scale. = TRUE)
# Plot PCA calculation
ggbiplot(pca, 
         obs.scale = 1, 
         var.scale = 1,
         groups = dataset$TenYearCHD, 
         circle = TRUE, alpha = 0.35) +
  scale_color_discrete(name = "Risk of CHD") +
  ggtitle("Principal Component Analysis")
```

In the plot above we can clearly see a contrast between the location of subjects with and without ten year risk of CHD. The subjects with risk of CHD cluster further along the x-axis than subjects lacking this risk. Since PC1 (x-axis) accounts for the larger amount of variation in our data (20.2%) differences along the x-axis are to be considered more significant than differences along the y-axis. We do, however, see some clustering along the y-axis as well. 

\newpage

### SMOTE

In an effort to balance out the instances of each of the class variable's labels SMOTE was applied. The initial imbalance and later parity can be observed in the form of this barplot. 

```{r SMOTE, fig.cap="Distribution of class variable pre-SMOTE and post-SMOTE."}

preSMOTE_CHD <- ggplot(dataset, 
                       aes(x = TenYearCHD, fill = TenYearCHD)
                       ) +
                      geom_bar(show.legend = TRUE) + 
                      ylab("Number of instances") +
                      theme(text=element_text(size=8)) + 
                      ggtitle("Pre-SMOTE distribution")

postSMOTE_CHD <- ggplot(postSMOTE_dataset, 
                        aes(x = TenYearCHD, fill = TenYearCHD)
                        ) +
                        geom_bar(show.legend = FALSE) + 
                        ylab("") +
                        theme(text=element_text(size=8), 
                              axis.text.y = element_blank(), 
                              axis.ticks.y = element_blank()
                              ) + 
                        ggtitle("Post-SMOTE distribution")

# arrange these boxplots into one figure
ggarrange(plotlist = list(preSMOTE_CHD, postSMOTE_CHD), 
          ncol = 2, 
          nrow = 1, 
          common.legend = TRUE, 
          legend = "bottom")
```

As can be observed in the pre-SMOTE distribution found on the left hand side, the number of instances labelled `no` outnumbered the `yes` labelled instances multiple times over. By synthetically creating more instances labelled `yes` these two classes were very nearly equalised, with the negative label still slightly outnumbering the positive label.

\newpage

## Validation & Performance

### Default settings

\begin{table}[thb]
\caption{\label{DefaultSMOTEComparison}Comparison of ML algorithms using default settings using 10-fold cross validation and using ZeroR as a baseline.}
\footnotesize
{\begin{tabular}{lr||r@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}c}\\
\hline
\textbf{Algorithm} & \textit{ZeroR} & \textit{OneR} & & \textit{J48} & & \textit{RandomForest} & & \textit{IBk} & & \textit{NaiveBayes} & & \textit{SimpleLogistic} & & \textit{SMO} & \\
\hline
\hline
\textbf{Elapsed time testing} & 0.00 & 0.00 &         & 0.00 &         & 0.03 & $\circ$ & 0.12 & $\circ$ & 0.00 & $\circ$ & 0.00 &         & 0.00 &        \\
\hline
\textbf{Accuracy} & 50.36 & 80.28 & $\circ$ & 74.83 & $\circ$ & 84.23 & $\circ$ & 77.06 & $\circ$ & 62.33 & $\circ$ & 67.36 & $\circ$ & 67.17 & $\circ$\\
\hline
\textbf{False Negative Rate} & 0.00 & 0.09 & $\circ$ & 0.26 & $\circ$ & 0.17 & $\circ$ & 0.31 & $\circ$ & 0.22 & $\circ$ & 0.33 & $\circ$ & 0.36 & $\circ$\\
\hline
\textbf{F-measure} & 0.67 & 0.82 & $\circ$ & 0.75 & $\circ$ & 0.84 & $\circ$ & 0.75 & $\circ$ & 0.67 &         & 0.67 &         & 0.66 &        \\
\hline
\textbf{ROC Area} & 0.50 & 0.80 & $\circ$ & 0.77 & $\circ$ & 0.92 & $\circ$ & 0.77 & $\circ$ & 0.70 & $\circ$ & 0.73 & $\circ$ & 0.67 & $\circ$\\
\hline
\multicolumn{16}{c}{$\circ$, $\bullet$ statistically significant increase or decrease}\\
\end{tabular} \footnotesize \par}
\end{table}

### Cost Sensitive Classification

\begin{table}[thb]
\caption{\label{DefaultSMOTEComparisonCSC}Comparison of ML algorithms using default settings and cost sensitive classification and 10-fold cross validation and using ZeroR as a baseline.}
\footnotesize
{\begin{tabular}{lr||r@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}c}\\
\hline
\textbf{Algorithm} & \textit{ZeroR} & \textit{OneR} & & \textit{J48} & & \textit{RandomForest} & & \textit{IBk} & & \textit{NaiveBayes} & & \textit{SimpleLogistic} & & \textit{SMO} & \\
\hline
\hline
\textbf{Elapsed time testing} & 0.00 & 0.00 &         & 0.00 &         & 0.03 & $\circ$ & 0.12 & $\circ$ & 0.00 & $\circ$ & 0.00 &         & 0.00 &        \\
\hline
\textbf{Accuracy} & 50.36 & 80.28 & $\circ$ & 74.65 & $\circ$ & 80.29 & $\circ$ & 77.06 & $\circ$ & 61.80 & $\circ$ & 61.95 & $\circ$ & 67.17 & $\circ$\\
\hline
\textbf{False Negative Rate} & 0.00 & 0.09 & $\circ$ & 0.23 & $\circ$ & 0.04 & $\circ$ & 0.31 & $\circ$ & 0.15 & $\circ$ & 0.12 & $\circ$ & 0.36 & $\circ$\\
\hline
\textbf{F-measure} & 0.67 & 0.82 & $\circ$ & 0.75 & $\circ$ & 0.83 & $\circ$ & 0.75 & $\circ$ & 0.69 & $\circ$ & 0.70 & $\circ$ & 0.66 &        \\
\hline
\textbf{ROC Area} & 0.50 & 0.80 & $\circ$ & 0.75 & $\circ$ & 0.80 & $\circ$ & 0.77 & $\circ$ & 0.62 & $\circ$ & 0.62 & $\circ$ & 0.67 & $\circ$\\
\hline
\multicolumn{16}{c}{$\circ$, $\bullet$ statistically significant increase or decrease}\\
\end{tabular} \footnotesize \par}
\end{table}

### Boosting

\begin{table}[thb]
\caption{\label{BoostingComp}Comparison of boosting with RandomForests using a varying number of boost iterations while using cost sensitive classification using 10-fold cross validation and using a RandomForest without boost as a baseline.}
\footnotesize
{\begin{tabular}{lr||r@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}cr@{\hspace{0.1cm}}c}
\hline
\textbf{Boost setting} & \textit{no boost} & \textit{-I 10} & & \textit{-I 25} & & \textit{-I 40} & & \textit{-I 55} &  \\
\hline
\hline
\textbf{Elapsed time testing} & 0.03 & 0.42 & $\circ$ & 1.25 & $\circ$ & 2.49 & $\circ$ & 3.65 & $\circ$\\
\hline
\textbf{Accuracy} & 80.29 & 85.25 & $\circ$ & 88.02 & $\circ$ & 88.84 & $\circ$ & 89.28 & $\circ$\\
\hline
\textbf{False Negative Rate} & 0.04 & 0.05 & $\circ$ & 0.06 & $\circ$ & 0.06 & $\circ$ & 0.06 & $\circ$\\
\hline
\textbf{F-measure} & 0.83 & 0.87 & $\circ$ & 0.89 & $\circ$ & 0.89 & $\circ$ & 0.90 & $\circ$\\
\hline
\textbf{ROC Area} & 0.80 & 0.94 & $\circ$ & 0.94 & $\circ$ & 0.93 & $\circ$ & 0.93 & $\circ$\\
\hline
\multicolumn{10}{c}{$\circ$, $\bullet$ statistically significant increase or decrease}\\
\end{tabular} \footnotesize \par}
\end{table}

\newpage

## Research findings

\begin{table}[thb]
\caption{\label{FinalModel}Final model which utilises boosting with RandomForests while using cost sensitive classification using 10-fold cross validation.}
\footnotesize
{\begin{tabular}{l||r@{\hspace{0.1cm}}} \\
\hline
\textbf{Final Model} & RandomForest  \\
\hline
\hline
\textbf{Accuracy} & 89.209 \\
\hline
\textbf{False Negative Rate} & 0.05 \\
\hline
\textbf{F-measure} & 0.892 \\
\hline
\textbf{ROC Area} & 0.942 \\
\hline
\end{tabular} \footnotesize \par}
\end{table}

\newpage

# Conclusion & Discussion

## Result based conclusions

## Discussion

## General conclusions and perspective

\newpage

# Project Proposal

\newpage

# Bibliography
