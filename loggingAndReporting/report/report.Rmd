---
title: "Predicting risk of Coronary Heart Disease using Machine Learning"
author: "Dennis Wiersma"
date: "`r Sys.Date()`"
urlcolor: blue
output: 
  pdf_document: 
    highlight: kate
    number_sections: yes
    citation_package: natbib
    
    includes: 
      in_header: preamble.tex
      before_body: prefix.tex
      
bibliography: references.bib
---

```{r, include=FALSE}
# Copyright (c) 2022 Dennis Wiersma.
# Licensed under GPLv3. See LICENSE file.
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# Excludes all code from the report
knitr::opts_chunk$set(include = FALSE)
```

```{r loadingDataset}
# Load dataset
dataset <- read.table(file = "../data/processedData.csv", header = TRUE, sep = ",", stringsAsFactors = TRUE)
```

\newpage

# Introduction

Heart disease is a condition that gets more prevalent by the day. It is one of the most, if not the most, occurring cause of death in this day and age [@World_Health_Organization_2020_who]. It is therefore important to know what factors can lead to someone developing heart disease. Thankfully, a lot is already known about what goes into developing this condition. So, if it is known what causes heart disease, then what can we do with this information? The answer is prevention. Possibly just as important as knowing what causes heart disease would be to know whether someone is likely to develop a heart condition in the near future. After all, if one knows they're at risk of succumbing to the number one killer in the world, they can still make an effort to subvert disaster. In the process of gathering such information a patient could seek advice from their physician, who could then evaluate their lifestyle and physical condition. Using this information the medical practitioner can then estimate whether the patient is likely to be subject to the unease of heart disease.\
Computer scientists wouldn't be computer scientists if they weren't trying to program someone out of a job, so what if this evaluation by a doctor could be automated by a computer. A piece of software could evaluate data from dozens of patients in mere seconds. This would cause the need for a physician to greatly decrease for this procedure, lowering both cost and barrier to entry.\
To achieve this dream of automation a range of techniques were used, the most important of which is Machine Learning (ML) which is a sub field of Artificial Intelligence (AI). Using these ML techniques a model can be trained using a large amount of data. This model can then predict certain values for a given input. For example, such a model could predict whether a patient has the risk of developing heart disease when given information about that patient's medical condition.\
As mentioned before, for the training of this ML model a rather large amount of data is required. The dataset which was used for completing this training phase was gathered from the Framingham Heart Study (FHS) and contains information on the risk of subjects developing Coronary Heart Disease (CHD) within the next ten years. Since these models can be fairly specific to the data on which they have been trained, any bias in the dataset will also translate into the model produced from it. This therefore leaves us with the following question to answer: How well can one predict 10 year risk of coronary heart disease in subjects from the Framingham Heart Study?

\newpage

# Methodology

For the successful execution of this project a collection of techniques and software were used instructions for which were provided by Hanze University of Applied Sciences [@Michiel_Noback_2022_hanze]. This, together with the process of going from raw data to the classification of brand new data, will be discussed in this chapter.

## Software

An overview of software, packages, and programming languages used for this project. The rest of this chapter will unfold when and where these various tools were used.

```{=tex}
\begin{table}[h]
\caption{Software used for this project.}
\begin{tabular}{l||rl}
\hline
Software & Version  & Function  \\ 
\hline
\hline
\textbf{R} & 4.2.1 & Statistical programming \\
\hline
\textbf{Java} & 17.0.5 & General purpose programming \\
\hline
\textbf{Weka} & 3.8.6 & Machine learning software workbench \\
\hline
\end{tabular}
\end{table}
```
```{=tex}
\begin{table}[h]
\caption{R packages used for this project.}
\begin{tabular}{l||rl}
\hline
R package & Version  & Function  \\ 
\hline
\hline
\textbf{knitr} & 1.40 & Dynamic report generation \\
\hline
\textbf{ggplot2} & 3.3.6 & Data visualisation \\
\hline
\textbf{ggpubr} & 0.4.0 & Arranging plots \\
\hline
\textbf{ggbiplot} & 0.55 & PCA plot \\
\hline
\textbf{corrplot} & 0.92 & Correlation plot \\
\hline
\textbf{RWeka} & 0.4-44 & Writing data to ARFF \\
\hline
\end{tabular}
\end{table}
```
```{=tex}
\begin{table}[h]
\caption{Java packages used for this project.}
\begin{tabular}{l||rl}
\hline
Java package & Version  & Function  \\ 
\hline
\hline
\textbf{Gradle} & 7.4 & Project management \\
\hline
\textbf{Weka API} & 3.8.0 & Weka programming interface \\
\hline
\textbf{ShadowJar} & 7.1.2 & Fat JAR building \\
\hline
\end{tabular}
\end{table}
```
## Data

As mentioned before, training a machine learning algorithms requires data. This data is a subset of the statistics gathered for the FHS and contains over four thousand instances and sixteen variables.

### Acquisition

Obtaining the data was a fairly easy process of hunting down an interesting dataset on the [Kaggle website](https://www.kaggle.com/) that fulfilled a certain set of requirements. These requirements include, but are not limited to:

* Choosing a supervised learning problem. This means the dataset in question is to contain class labels.
* The set contains data from the life sciences. This includes, for example, biology, chemistry, and medicine.
* At least seven attributes are present in the dataset, but preferably more.
* At least several hundred instances are present in the dataset, but preferably more.

At the close of hunting season a Kaggle page called "Logistic regression To predict heart disease" [@Dileep_2019_kaggle] was chosen for this project. It is the page that contains the subset of data from the FHS. One thing to note about this Kaggle page is that nowhere does it mention how they acquired this data from the FHS project. It simply links to another, by now defunct, Kaggle page. Apart from that it just has a download link to the data, some information, and a not that comprehensive analysis.

### Processing

After acquiring the dataset, the first thing is of course to load it into one's software package of choice, which in this case would be R. Once the data has been loaded it is time to take a good look at it, and it is at this point that the first problem arises. This collection of data contains an `education` attribute consisting of values ranging from one through to four. An explanation of what these numbers represent is nowhere to be found. It was therefore decided to drop this variable, since it is not entirely clear what exactly it represents.

Moving on one runs into the second issue, which are the missing units. Just like a description of the `education` characteristic was missing, a lot of units for the given measurements were missing. Reading the Kaggle page and scouring the FHS website yielded no results, but luckily some of the missing units were easily interpreted.

Now that the missing metadata has been dealt with it was time to correctly label the data. Some of the data consisted of binary nominal values but were encoded in numerics. These variables were transformed into R's factor objects. These factors consist of levels like `male` and `female`, or `yes` and `no`. These are perfect for encoding nominal attributes.

While the missing metadata and wrongly encoded data have been resolved, the missing values in our dataset have not been taken care of yet. Resolving these so called `NA` values will be done by reviewing them on an attribute by attribute basis. \
First up are the amount of cigarettes smoked per day by a given subject. The initial step was to check whether every instance that had a missing value to their name was currently a smoker. This turned out to be the case, which meant these values could be imputed from the remaining values. This imputation was done by taking the mean number of cigarettes smoked \textit{by subjects that are currently smokers} and assigning this mean value to the gaps in our data. \
Next were the missing values for whether the subject was or wasn't on blood pressure medication. After looking at the dataset it was determined that, as one might suspect, a very large majority of subjects were in fact not on blood pressure medication. It was therefore assumed that the instances with the missing values would not be on these types of medication either. \
The following attributes were dealt with in bulk since the process of imputation was precisely identical for each of them. The variables in question are:

* Total cholesterol levels
* BMI
* Heart rate
* Glucose levels

Since all of these attributes consist of plain integer data they can easily be imputed. It was decided that the imputation process would simply consist of assigning the missing values the respective attribute's mean values.

Last but not least there's the problem regarding our so called class variable. This is the attribute which tells us whether a subject actually does run the risk of developing CHD in the next ten years. This attribute turned out to be heavily skewed towards subjects without said risk. This could potentially pose a myriad of issues during the training of our ML model. This issue was solved using Synthetic Minority Oversampling Technique (SMOTE). This is an approach where new (synthetic) instances are created which are labelled with the minority class, which would be the `yes` value in this case. 
\newpage

## Machine Learning

Now that the data was ready, it was finally time to start work on the ML model. A lot of the work of exploring different algorithms, comparing different algorithms, comparing different hyper parameters, and figuring out what worked best was done using the Weka ML software workbench. 

### Performance

When trying to develop an ML model there are hordes of different algorithms to choose from, each of which perform better when applied in their own specific use case. So then, how does one figure out which algorithm works best for the data at hand? This is where the Weka experimenter comes in. This section of the Weka workbench lets users easily test and compare algorithms and parameters. \
Apart from finding a software solution, another question to answer is which scoring methods to use when making such comparisons. In the case of this research project this encompasses the following scoring methods:

* Classification speed
* Accuracy
* False Negative Rate (FNR)
* F-measure
* Area Under the Curve of Receiver Operating Characteristic (AUC-ROC)

These metrics will be used to compare different algorithms, but which algorithms to compare? ML algorithms can be divided into different categories. A few examples of these categories would be rule based, tree based, and lazy learning based algorithms. A selection of algorithms to compare has been carefully curated by ensuring every category of was represented. This resulted in the following algorithms being compared:

* ZeroR
* OneR
* J48
* Random Forest
* IBk
* Naive Bayes
* Simple Logistic
* Sequential Minimal Optimization (SMO)

The results of this comparison prompted the application of Cost Sensitive Classification (CSC). This is a technique where a cost can be applied to instances which are classified a certain way. The classifications that CSC can be applied to are True Positive (TP), True Negative (TN), False Positive (FP), and False Negative (FN) designations. In the context of this research a FN allotment regards a subject that does run the risk of developing CHD but would not be classified as such. It would be best to minimise this outcome, even if it means classifying a number of negative instances as positive. The final cost matrix can be found below:

\begin{table}[h]
\caption{Cost matrix where FN instances are punished more.}
  \begin{tabular}{ |c|c||c|c|  }
    \hline
    & \multicolumn{3}{c|}{\textbf{Predicted condition}} \tabularnewline
    \hline
    & & \textbf{Positive} & \textbf{Negative} \\
    \hline
    \hline
    \multirow{3}{*}{\rotatebox{90}{\textbf{Actual condition \hspace{1em}}}} & & & \\
    & & & \\
    & \textbf{Positive} & 0 (TP) & 2.5 (FN) \\ 
    & & & \\
    & & & \\
    \cline{2-4}
    & & & \\
    & & & \\
    & \textbf{Negative} & 1 (FP) & 0 (TN) \\
    & & & \\
    & & & \\
    \hline
  \end{tabular}
\label{table:confMatrix}
\end{table}

\newpage

### Interfacing

\newpage

# Results

## Data acquisition & preprocessing

## Validation & Performance

## Research findings

\newpage

# Conclusion & Discussion

## Result based conclusions

## Discussion

## General conclusions and perspective

\newpage

# Bibliography
